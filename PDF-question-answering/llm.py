from typing import Optional, Union, List
from google import genai
from google.genai import types
import base64


def llm_setup(project: str, location: str) -> genai.Client:
    """Initializes and returns the GenAI client with Vertex AI."""
    try:
        return genai.Client(vertexai=True, project=project, location=location)
    except Exception as e:
        raise RuntimeError(f"Failed to initialize GenAI client: {e}") from e


def llm_config_setup(
    sys_prompt: Optional[str] = None,
    temperature: float = 0.7,
    seed: int = 0,
    max_output_tokens: int = 65535
) -> types.GenerateContentConfig:
    """Creates and returns a GenerateContentConfig object with safety and system settings."""
    config_dict = {
        "temperature": temperature,
        "seed": seed,
        "max_output_tokens": max_output_tokens,
        "safety_settings": [
            types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="OFF"),
            types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="OFF"),
            types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="OFF"),
            types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="OFF"),
        ],
        "thinking_config": types.ThinkingConfig(thinking_budget=-1),
    }

    if sys_prompt:
        config_dict["system_instruction"] = [types.Part.from_text(text=sys_prompt)]

    return types.GenerateContentConfig(**config_dict)


def get_llm_response(
    client: genai.Client,
    generate_content_config: types.GenerateContentConfig,
    usr_prompt: str,
    contents: List,
    model: str = "gemini-2.5-flash"
) -> tuple[str, List[types.Content]]:
    """
    Generates a response from the LLM using the given context and prompt.

    Returns:
        response_text (str): The model's response.
        updated_contents (List[types.Content]): Updated conversation history.
    """
    if not usr_prompt.strip():
        raise ValueError("User prompt must not be empty.")

    # Append new user message to conversation history
    contents.append(types.Content(role="user", parts=[types.Part.from_text(text=usr_prompt)]))

    try:
        raw_response = client.models.generate_content(
            model=model,
            contents=contents,
            config=generate_content_config
        )
        response_text = raw_response.candidates[0].content.parts[0].text

        contents.append(types.Content(role="model", parts=[types.Part.from_text(text=response_text)]))
        
        return response_text, contents
    except Exception as e:
        raise RuntimeError(f"Failed to generate LLM response: {e}") from e


def pdf_to_text_using_vision_llm(
    client: genai.Client,
    generate_content_config: types.GenerateContentConfig,
    document,
    model: str = "gemini-2.5-flash"
) -> str:
    """
    Generates a response from the LLM using the given prompt and document context.

    Args:
        client: Initialized GenAI client.
        generate_content_config: LLM configuration.
        document: Optional base64-encoded PDF string.
        model: LLM model name.

    Returns:
        response_text: Generated response.
    """
    if not document:
        raise ValueError("Document must not be empty.")

    try:
        document_part = types.Part.from_bytes(
            data=base64.b64decode(document),
            mime_type="application/pdf"
        )
    except Exception as e:
        raise RuntimeError(f"Failed to decode document: {e}") from e

    contents = [
        types.Content(
            role="user",
            parts=[
                document_part,
                types.Part.from_text(text="Extract everything as text from this document.")
            ]
        )
    ]
    try:
        response = client.models.generate_content(
            model=model,
            contents=contents,
            config=generate_content_config
        )
        if not response.candidates or not response.candidates[0].content.parts:
            raise ValueError("No valid response generated by LLM.")

        return response.candidates[0].content.parts[0].text

    except Exception as e:
        raise RuntimeError(f"Failed to generate LLM response: {e}") from e



# def pdf_to_text_using_vision_llm_():

#     from google.cloud import vision

#     client = vision.ImageAnnotatorClient()
#     feature = vision.Feature(
#         type_=vision.Feature.Type.DOCUMENT_TEXT_DETECTION
#     )
#     input_config = vision.InputConfig(
#                     gcs_source=vision.GcsSource(uri=gcp_file_path),
#                     mime_type='application/pdf'
#                 )
#     output_uri = f"gs://{bucket.name}/extracted_data_from_pdf/{document_name.strip('.pdf')}/"
#     output_config = vision.OutputConfig(
#             gcs_destination=vision.GcsDestination(uri=output_uri),
#             batch_size=1
#         )
#     async_request = vision.AsyncAnnotateFileRequest(
#             features=[feature],
#             input_config=input_config,
#             output_config=output_config
#         )
#     operation = client.async_batch_annotate_files(
#             requests=[async_request]
#         )
#     return